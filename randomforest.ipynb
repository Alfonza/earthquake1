{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-29-1619602cebeb>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-1619602cebeb>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    compair to other\u001b[0m\n\u001b[1;37m                     \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "''''Random forests is a supervised learning algorithm. It can be used both for classification and regression. \n",
    "It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more \n",
    "trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, \n",
    "gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator \n",
    "of the feature importance.\n",
    "\n",
    "It works in four steps:\n",
    "\n",
    "*Select random samples from a given dataset.\n",
    "*Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
    "*Perform a vote for each predicted result.\n",
    "*Select the prediction result with the most votes as the final prediction.\n",
    "................................................\n",
    "Advantages: Random forests is considered as a highly accurate and robust method because of the number of decision trees \n",
    "            participating in the process.\n",
    "            It does not suffer from the overfitting problem. The main reason is that it takes the average of all the \n",
    "            predictions, which cancels out the biases.\n",
    "            The algorithm can be used in both classification and regression problems.\n",
    "            Random forests can also handle missing values. (There are two ways to handle these: using median values to replace \n",
    "            continuous variables, and computing the proximity-weighted average of missing values.) \n",
    "            we can get the relative feature importance, which helps in selecting the most contributing features for the \n",
    "            classifier.\n",
    "Disadvantages: Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a\n",
    "               prediction, all the trees in the forest have to make a prediction for the same given input and then perform \n",
    "               voting on it. This whole process is time-consuming.\n",
    "              The model is difficult to interpret compared to a decision tree, where you can easily make a decision by \n",
    "              following the path in the tree\n",
    "...............................................\n",
    "compair to other '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Building_Ownership_Use.csv', 'Building_Structure.csv', 'ReadMe.md', 'sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "#open the dataset files and list the directories available in that dataset file\n",
    "path=\"C:\\\\Users\\\\best\\\\Downloads\\\\Earthquake_damage\\\\Earthquake_damage\"\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the csv file ,take first 10000 data from total data.pd.read_csv is used to read the csv file and do operations on it\n",
    "training=pd.read_csv(path+'\\\\train.csv')      \n",
    "training.head()\n",
    "data1=training[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv(path+'\\\\Building_Ownership_Use.csv')\n",
    "data2.head()\n",
    "data3 = pd.merge(data2,data1,on=['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=pd.read_csv(path+'\\\\Building_Structure.csv')\n",
    "data4.head()\n",
    "data= pd.merge(data4,data3,on=['building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building_id', 'district_id', 'vdcmun_id', 'ward_id_x', 'count_floors_pre_eq', 'count_floors_post_eq', 'age_building', 'plinth_area_sq_ft', 'height_ft_pre_eq', 'height_ft_post_eq', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', 'other_floor_type', 'position', 'plan_configuration', 'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', 'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', 'condition_post_eq', 'district_id_x', 'vdcmun_id_x', 'ward_id_y', 'legal_ownership_status', 'count_families', 'has_secondary_use', 'has_secondary_use_agriculture', 'has_secondary_use_hotel', 'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school', 'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', 'has_secondary_use_other', 'area_assesed', 'damage_grade', 'district_id_y', 'has_geotechnical_risk', 'has_geotechnical_risk_fault_crack', 'has_geotechnical_risk_flood', 'has_geotechnical_risk_land_settlement', 'has_geotechnical_risk_landslide', 'has_geotechnical_risk_liquefaction', 'has_geotechnical_risk_other', 'has_geotechnical_risk_rock_fall', 'has_repair_started', 'vdcmun_id_y']\n"
     ]
    }
   ],
   "source": [
    "#list the what are the featuers consider for training\n",
    "features=list(data)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary for storing the  numeric value of corresponding string \n",
    "dictionary={}\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertion is a function used to convert the string values in to numeric value ,where we take word is the key,\n",
    "def convertion(dictionary,word):\n",
    "    if word not in dictionary:\n",
    "        value=len(dictionary)\n",
    "        dictionary[word]=value\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return dictionary[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through each cell ,that cell contain value stored into variable word.then find the datatype of each cell values ,\n",
    "#if it is objective then call convertion function.then we get the neumeric value corresponding object.then it consider the new\n",
    "#new value of the that perticular cell\n",
    "i=0\n",
    "for i in range(len(data)):\n",
    "    for feature in features:\n",
    "        word=data.loc[i,feature]\n",
    "        if data[feature].dtype==object:\n",
    "            convertion(dictionary,word)\n",
    "            new_data=dictionary[word]\n",
    "            data.loc[i,feature]=new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the dataset any cell is empty then it replace with 0\n",
    "data.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create two class with or without grade.so that in x contain dataset without grade and y contain with label.for \n",
    "#training purpose we make this category\n",
    "x = data.drop('damage_grade', axis=1)\n",
    "y = data['damage_grade']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want training and testing dataset for that we splite the available dataset by using train_test_split.spliting done with 80% training 20% for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_Train, x_Test, y_Train, y_Test = train_test_split(x, y, test_size = 0.2,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_Train,y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(x_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 346  138    0  264    2]\n",
      " [ 192  264    0   82    5]\n",
      " [   7    2 1272   69    0]\n",
      " [ 189   54    4  683    1]\n",
      " [  28   49    0    4  345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          11       0.45      0.46      0.46       750\n",
      "          17       0.52      0.49      0.50       543\n",
      "          27       1.00      0.94      0.97      1350\n",
      "          30       0.62      0.73      0.67       931\n",
      "          35       0.98      0.81      0.89       426\n",
      "\n",
      "    accuracy                           0.73      4000\n",
      "   macro avg       0.71      0.69      0.70      4000\n",
      "weighted avg       0.74      0.73      0.73      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#At this point we have trained our algorithm and made some predictions. Now we'll see how accurate our algorithm is. \n",
    "#For classification tasks some commonly used metrics are confusion matrix, precision, recall, and F1 score. \n",
    "#Lucky for us Scikit=-Learn's metrics library contains the classification_report and confusion_matrix methods that can be used \n",
    "#to calculate these metrics for us:\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_Test, y_pred))\n",
    "print(classification_report(y_Test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
